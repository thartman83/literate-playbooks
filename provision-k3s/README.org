-*- mode: org; mode: org-make-toc-mode -*-
#+TITLE: Provision K3S Cluster
#+AUTHOR: Tom Hartman
#+STARTUP: overview
* Table of Contents
:PROPERTIES:
:TOC:      :include all :ignore this
:END:
:CONTENTS:
- [[#general][General]]
- [[#inventory][Inventory]]
- [[#host-variables][Host Variables]]
- [[#playbook-definitions][Playbook Definitions]]
  - [[#roles][Roles]]
    - [[#setup-k3s-on-controller][Setup k3s on controller]]
:END:

* General
Provision a new k3s cluster on listed hosts

* Inventory

#+begin_src ini :tangle inventory/hosts.ini
[controller]
boreas

[workers]
notus
eurus
zephyrus

[nodes]
boreas
notus
eurus
zephyrus
#+end_src

* Group Variables
#+begin_src yaml :tangle group_vars/all
k3spasswd: k3spasswd
net_prefix: 172.17.1.
k3s_primary_controller: boreas
k3s_nodes:
  - boreas
  - notus
  - eurus
  - zephyrus

kube_config_dir: /root/.kube
helm_src: https://get.helm.sh/helm-v3.12.3-linux-amd64.tar.gz
helm_archive: helm-v3.12.3-linux-amd64.tar.gz
helm_checksum: 1b2313cd198d45eab00cc37c38f6b1ca0a948ba279c29e322bdf426d406129b5
helm_gpgkey: "672C 657B E06B 4B30 969C 4A57 4614 49C2 5E36 B98E"

metallb_helm_repourl: https://metallb.github.io/metallb
metallb_namespace: metallb-system
metallb_iprange: 172.17.1.60-172.17.1.69

longhorn_storage_dir: /storage
longhorn_helm_repourl: https://charts.longhorn.io
longhorn_namespace: longhorn-system
longhorn_loadbalancer_ip: 172.17.1.61

cert_manager_repourl: https://charts.jetstack.io
cert_manager_loadbalancer_ip: 172.17.1.63
cert_manager_namespace: cert-manager
cert_manager_version: v1.13.0

docker_reg_label: docker-registry
docker_reg_cert_localpath: ~/cert
docker_reg_cert_name: registry-cert
docker_reg_namespace: docker-registry
docker_reg_pvc_name: docker-registry-pvc
docker_reg_pvc_size: 5Gi
docker_reg_replicas: 1
docker_reg_container_port: 5000
docker_reg_tls_crt: ~/certs/olympus/olympus.docker-registry.crt
docker_reg_tls_key: ~/certs/olympus/olympus-root.key
docker_reg_service_name: docker-registry-service
docker_reg_service_ip: 172.17.1.62
#+end_src

* Host Variables

* Playbook Definition

#+begin_src yaml :tangle provision-k3s.yml
---
- name: Provision a new k3s cluster controller
  hosts: controller
  roles:
    - role: k3s-controller
  tags: k3s-controller

- name: Provision the k3s cluter workers
  hosts: workers
  roles:
    - role: k3s-worker
  tags: k3s-workers

- name: Provision nodes as workers roles
  hosts: controller
  roles:
    - role: k3s-worker-roles
  tags:
    - k3s-workers

- name: Provision helm on the controller
  hosts: controller
  roles:
    - role: k3s-helm-controller
  tags:
    - helm

- name: Provision metallb loadbalancer on the cluster
  hosts: controller
  roles:
    - role: k3s-loadbalancer
  tags: loadbalancer

- name: Provision longhorn storage on all works
  hosts: nodes
  roles:
    - role: k3s-storage-common
  tags:
    - longhorn-common
    - longhorn

- name: Provision longhorn controller
  hosts: controller
  roles:
    - role: k3s-storage-controller
  tags:
    - longhorn-controller
    - longhorn

- name: Cert Manager
  hosts: controller
  roles:
    - role: k3s-cert-manager
  tags:
    - cert-manager

- name: Docker registry
  hosts: controller
  roles:
    - role: k3s-docker-registry
  tags:
    - docker-registry
#+end_src

* Roles

** Setup k3s on controller

First step is to install k3s on the controllers. First we check to see if the controller already has k3s installed. Running which seems to be the simplest way to check this though I wish there was a built in command to accomplish this rather than having to do stuff like ignoring errors and returning an rc value.

#+begin_src yaml :tangle roles/k3s-controller/tasks/main.yml
- name: Check if k3s is already installed on the controller
  tags: k3s-controller
  command: which k3s
  register: which_k3s
  ignore_errors: True
  changed_when: "which_k3s.rc == 1"
#+end_src

K3s will need to know what the ip address is of the controller

#+begin_src yaml :tangle roles/k3s-controller/tasks/main.yml
- name: Get the ip address
  tags: k3s-controller
  shell:
    cmd: |-
      ip a | sed -n '{{ ipaddr_regex }}'
  vars:
    ipaddr_regex: 's/.*inet \({{ net_prefix }}[0-9]*\).*/\1/p'
  register: ipaddr
  when: "which_k3s.rc == 1"
#+end_src

Again doing this using commands rather than built commands feels a little off but it works.

And now we do something we know we shouldn't do and curl directly into a subshell but again this is the normal installation method unfortunately.

#+begin_src yaml :tangle roles/k3s-controller/tasks/main.yml
- name: Provision the controller nodes
  tags: k3s-controller
  shell:
    cmd: |-
      curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --disable servicelb --token "{{ k3spasswd }}" --node-ip "{{ ipaddr.stdout }}" --disable-cloud-controller --disable local-storage
  when: "which_k3s.rc == 1"

- name: Reboot controller nodes
  tags: k3s-controller
  reboot:
    reboot_timeout: 1000
  when: "which_k3s.rc == 1"
#+end_src

We have to follow up the installation with a reboot unfortunately but once that is done we should see the controller show up as a node in kubectl.

#+begin_src yaml :tangle roles/k3s-controller/tasks/main.yml
- name: Check that the control plane is running
  tags: k3s-controller
  command: kubectl get nodes
  register: kubectl_nodes_data
  failed_when: (kubectl_nodes_data.stdout | regex_search(control_plane_regex, multiline=True)) == ""
  vars:
    control_plane_regex: '{{ inventory_hostname }}.*Ready.*control-plane,master'
#+end_src

In order to run k8s ansible tasks the controller will need the following additional packages installed: python-kubernetes, pyYaml, and jsonpatch.

#+begin_src yaml :tangle roles/k3s-controller/tasks/main.yml
- name: Install required packages for ansible k8s module
  tags: k3s-controller
  package:
    name:
      - python3-pip

- name: Install python packages
  tags: k3s-controller
  pip:
    name:
      - kubernetes
      - pyYAML
      - jsonpatch
    executable: pip3
#+end_src

** Setup k3s on the worker
Now we need to install k3s on each of the workers from the controller if it is not already present

#+begin_src yaml :tangle roles/k3s-worker/tasks/main.yml
- name: Check if k3s is already installed on the worker
  tags: k3s-workers
  command: which k3s
  register: which_k3s
  ignore_errors: True
  changed_when: "which_k3s.rc == 1"
#+end_src

#+begin_src yaml :tangle roles/k3s-worker/tasks/main.yml
- name: Provision the controller nodes
  tags: k3s-workers
  shell:
    cmd: |-
      curl -sfL https://get.k3s.io | K3S_URL=https://{{ k3s_primary_controller }}:6443 K3S_TOKEN={{ k3spasswd }} sh -
  when: "which_k3s.rc == 1"

- name: Reboot the worker node
  reboot:
    reboot_timeout: 1000
  when: "which_k3s.rc == 1"
#+end_src

** Let workers be workers

We now need to use the controller to mark all of the nodes as viable workers.

#+begin_src yaml :tangle roles/k3s-worker-roles/tasks/main.yml
- name: Add worker node type for all nodes
  tags: k3s-workers
  command: kubectl label node {{ item }} kubernetes.io/role=worker
  loop: "{{ k3s_nodes }}"
#+end_src

#+begin_src yaml :tangle roles/k3s-worker-roles/tasks/main.yml
- name: Add worker node type for all nodes
  tags: k3s-workers
  command: kubectl label node {{ item }} node-type=worker
  loop: "{{ k3s_nodes }}"
#+end_src

** Controller Helm

Make sure that helm is installed on the controller, first we will need to check that git is available

#+begin_src yaml :tangle roles/k3s-helm-controller/tasks/main.yml
- name: The helm controller will need git installed
  package:
    name:
      - git

- name: Ensure that helm is installed
  command: which helm
  register: which_helm
  ignore_errors: True

#+end_src

For the moment we are using which command to check that the executable is available. This can probably be accomplish by using stat and looping over the environment path but for the moment this will do.

Setup a directory for kube configuration that helm will use locally. We export the kubectl configuration and then link it in /etc/environment.

#+begin_src yaml :tangle roles/k3s-helm-controller/tasks/main.yml
- name: Setup the kube configuration directory
  file:
    path: "{{ kube_config_dir }}"
    state: directory

- name: Grab the kubectl config
  command: k3s kubectl config view --raw
  register: kube_config

- name: Create the config file
  copy:
    content: "{{ kube_config.stdout }}"
    dest: "{{ kube_config_dir }}/config"
    mode: 600

- name: Add the kube config into the environment
  lineinfile:
    path: /etc/environment
    line: "KUBECONFIG={{ kube_config_dir }}/config"
#+end_src

Download the helm package and verify. Then move the executable into /usr/local/bin.

#+begin_src yaml :tangle roles/k3s-helm-controller/tasks/main.yml
- name: Download the helm source
  get_url:
    url: "{{ helm_src }}"
    dest: "/tmp"
    checksum: "sha256:{{ helm_checksum }}"
  when: "which_helm.rc == 1"

- name: Unarchive the helm source
  unarchive:
    src: "/tmp/{{ helm_archive }}"
    dest: "/tmp/"
    remote_src: True
  when: "which_helm.rc == 1"

- name: Move helm into usr/local/bin
  copy:
    remote_src: True
    src: /tmp/linux-amd64/helm
    dest: /usr/local/bin/
    mode: 700
  when: "which_helm.rc == 1"
#+end_src

** Load Balancer

Add metal load balancer to the cluster.

Add the repository url

#+begin_src yaml :tangle roles/k3s-loadbalancer/tasks/main.yml
- name: Add helm metallb chart repository
  kubernetes.core.helm_repository:
    name: metallb
    repo_url: "{{ metallb_helm_repourl }}"

- name: Deploy metal loadbalancer to the cluster
  kubernetes.core.helm:
    name: metallb
    chart_ref: metallb/metallb
    release_namespace: "{{ metallb_namespace }}"
    create_namespace: True
    wait: True
    update_repo_cache: True
#+end_src

#+begin_src yaml :tangle roles/k3s-loadbalancer/templates/metallb-ippool.yml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default-pool
  namespace: {{ metallb_namespace }}
spec:
  addresses:
  - {{ metallb_iprange }}
#+end_src

#+begin_src yaml :tangle roles/k3s-loadbalancer/templates/metallb-l2advertisement.yml
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: default
  namespace: {{ metallb_namespace }}
spec:
  ipAddressPools:
  - default-pool
#+end_src

#+begin_src yaml :tangle roles/k3s-loadbalancer/tasks/main.yml
- name: Apply metallb ip address pool configuration
  k8s:
    definition: "{{ lookup('template','templates/metallb-ippool.yml') | from_yaml }}"
#+end_src

#+begin_src yaml :tangle roles/k3s-loadbalancer/tasks/main.yml
- name: Apply metallb L2 Advertisement
  k8s:
    definition: "{{ lookup('template', 'templates/metallb-l2advertisement.yml') | from_yaml }}"
#+end_src

** Longhorn
For all nodes in the cluster prepare the storage location and install packages that longhorn will need for provisioning storage across the nodes.

#+begin_src yaml :tangle roles/k3s-storage-common/tasks/main.yml
- name: Create the storage directory if it does not exist
  ansible.builtin.file:
    path: "{{ longhorn_storage_dir }}"
    state: directory

- name: Install common nfs prereqs on all nodes
  package:
    name:
      - nfs-common
      - open-iscsi
      - util-linux
    state: present
#+end_src

#+begin_src sh
helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --set defaultSettings.defaultDataPath="/storage" --set service.ui.loadBalancerIP="10.0.0.71" --set service.ui.type="LoadBalancer"
#+end_src

#+begin_src yaml :tangle roles/k3s-storage-controller/tasks/main.yml
- name: Install Longhorn IO prereqs on all nodes
  apt:
    pkg:
      - python3-pip
    state: present

- name: Install PyYAML python package
  ansible.builtin.pip:
    name: PyYAML

- name: Add longhorn repository to the controller
  kubernetes.core.helm_repository:
    name: longhorn
    repo_url: "{{ longhorn_helm_repourl }}"

- name: Install longhorn via helm
  kubernetes.core.helm:
   name: longhorn
   chart_ref: longhorn/longhorn
   release_namespace: "{{ longhorn_namespace }}"
   create_namespace: true
   update_repo_cache: True
   set_values:
     - value: service.ui.loadBalancerIP="{{ longhorn_loadbalancer_ip }}"
       value_type: string
     - value: service.ui.type=LoadBalancer
       value_type: string
     - value: defaultSettings.defaultDataPath="{{ longhorn_storage_dir }}"
       value_type: string

#+end_src

** Cert-Manager
#+begin_src yaml :tangle roles/k3s-cert-manager/tasks/main.yml
- name: Add jetstack repository to the controller
  kubernetes.core.helm_repository:
    name: jetstack
    repo_url: "{{ cert_manager_repourl }}"

- name: Install cert-manager via helm
  kubernetes.core.helm:
   name: cert-manager
   chart_ref: jetstack/cert-manager
   release_namespace: "{{ cert_manager_namespace }}"
   create_namespace: true
   update_repo_cache: True
   set_values:
     - value: installCRDs=true
       value_type: string
#+end_src

#+begin_src yaml :tangle roles/k3s-cert-manager/templates/cert-manager-issuer.yml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: {{ cert_manager_namespace }}-issuer
  namespace: {{ cert_manager_namespace }}
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: {{ cert_manager_namespace }}-ca
  namespace: {{ cert_manager_namespace }}
spec:
  isCA: true
  commonName: {{ cert_manager_namespace }}-ca
  secretName: ca-secret
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: {{ cert_manager_namespace }}-issuer
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: ca-issuer
spec:
  ca:
    secretName: ca-secret
#+end_src

#+begin_src yaml :tangle roles/k3s-cert-manager/tasks/main.yml
- name: Create the issuer for cert-manager
  k8s:
    definition: "{{ lookup('template', 'templates/cert-manager-issuer.yml') }}"

#+end_src

** Docker Registry

Create the docker registry namespace
#+begin_src yaml :tangle roles/k3s-docker-registry/tasks/main.yml
- name: Create a docker registry namespace
  kubernetes.core.k8s:
    name: "{{ docker_reg_namespace }}"
    api_version: v1
    kind: Namespace
    state: present
#+end_src

#+begin_src yml :tangle roles/k3s-docker-registry/templates/docker-registry-cert.yml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: {{ docker_reg_namespace }}-certificate
  namespace: {{ docker_reg_namespace }}
spec:
  secretName: {{ docker_reg_namespace }}-secret
  dnsNames:
  - "*.{{ docker_reg_namespace }}.svc.cluster.local"
  - "*.{{ docker_reg_namespace }}"
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer
#    group: cert-manager.io/v1
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/tasks/main.yml
- name: Create the issuer for cert-manager
  k8s:
    definition: "{{ lookup('template', 'templates/docker-registry-cert.yml') }}"

#+end_src

Create a persistent volume for the registry

#+begin_src yaml :tangle roles/k3s-docker-registry/templates/registry-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ docker_reg_pvc_name }}
  namespace: {{ docker_reg_namespace }}
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: {{ docker_reg_pvc_size }}
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/tasks/main.yml
- name: Create persistent volume for the registry
  k8s:
    definition: "{{ lookup('template', 'templates/registry-pvc.yml') | from_yaml }}"
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/templates/cert-secrets.yml
apiVersion: v1
data:
  tls.crt: {{ lookup('file', docker_reg_tls_crt ) | b64encode }}
  tls.key: {{ lookup('file', docker_reg_tls_key ) | b64encode }}
kind: Secret
metadata:
  name: {{ docker_reg_cert_name }}
  namespace: {{ docker_reg_namespace }}
type: kubernetes.io/tls
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/tasks/main.yml
- name: Add secrets
  k8s:
    definition: "{{ lookup('template', 'templates/cert-secrets.yml') | from_yaml }}"
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/templates/registry-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: {{ docker_reg_label }}
  name: {{ docker_reg_label }}
  namespace: {{ docker_reg_namespace }}
spec:
  replicas: {{ docker_reg_replicas }}
  selector:
    matchLabels:
      run: {{ docker_reg_label }}
  template:
    metadata:
      labels:
        run: {{ docker_reg_label }}
        app: {{ docker_reg_label }}
    spec:
      nodeSelector:
        node-type: worker
      containers:
      - name: {{ docker_reg_label }}
        image: registry:2
        ports:
        - containerPort: {{ docker_reg_container_port }}
        env:
        - name: REGISTRY_HTTP_TLS_CERTIFICATE
          value: "/certs/tls.crt"
        - name: REGISTRY_HTTP_TLS_KEY
          value: "/certs/tls.key"
        volumeMounts:
        - name: docker-registry-secret
          mountPath: "/certs"
          readOnly: true
        - name: registry-data
          mountPath: /var/lib/registry
          subPath: registry
      volumes:
      - name: docker-registry-secret
        secret:
          secretName: docker-registry-secret
      - name: registry-data
        persistentVolumeClaim:
          claimName: {{ docker_reg_pvc_name }}
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/tasks/main.yml
- name: Deploy the registry to the cluster
  k8s:
    definition: "{{ lookup('template', 'templates/registry-deployment.yml') | from_yaml }}"
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/templates/registry-service.yml
apiVersion: v1
kind: Service
metadata:
  name: {{ docker_reg_service_name }}
  namespace: {{ docker_reg_namespace }}
  annotations:
    metallb.universe.tf/address-pool: default-pool
spec:
  selector:
    app: {{ docker_reg_label }}
  ports:
  - port: 5000
    targetPort: 5000
  type: LoadBalancer
  loadBalancerIP: {{ docker_reg_service_ip }}
#+end_src

#+begin_src yaml :tangle roles/k3s-docker-registry/tasks/main.yml
- name: Deploy the registry service
  k8s:
    definition: "{{ lookup('template', 'templates/registry-service.yml') | from_yaml }}"
#+end_src
